{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary workaround for MLX-975\n",
    "#In utils/hive-site.xml edit hive.metastore.warehouse.dir and hive.metastore.warehouse.external.dir based on settings in CDP Data Lake -> Cloud Storage\n",
    "import os, shutil\n",
    "if ( not os.path.exists('/etc/hadoop/conf/hive-site.xml')):\n",
    "  shutil.copyfile(\"/home/cdsw/utils/hive-site.xml\", \"/etc/hadoop/conf/hive-site.xml\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Airline ML\")\\\n",
    "    .config(\"spark.executor.memory\",\"16g\")\\\n",
    "    .config(\"spark.executor.cores\",\"4\")\\\n",
    "    .config(\"spark.driver.memory\",\"6g\")\\\n",
    "    .config(\"spark.executor.instances\",\"5\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://ml-field/demo/flight-analysis/data/\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\\\n",
    "#.config(\"spark.hadoop.fs.s3a.metadatastore.impl\",\"org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore\")\\\n",
    "#.config(\"spark.hadoop.fs.s3a.delegation.token.binding\",\"\")\\\n",
    "\n",
    "flight_df=spark.read.parquet(\n",
    "  \"s3a://ml-field/demo/flight-analysis/data/airline_parquet_2/\",\n",
    ")\n",
    "\n",
    "flight_df = flight_df.na.drop().limit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf,substring\n",
    "\n",
    "convert_time_to_hour = udf(lambda x: x if len(x) == 4 else \"0{}\".format(x),StringType())\n",
    "#df.withColumn('COLUMN_NAME_fix',udf1('COLUMN_NAME')).show()\n",
    "\n",
    "flight_df = flight_df.withColumn('CRS_DEP_HOUR', substring(convert_time_to_hour(\"CRS_DEP_TIME\"),0,2))\n",
    "flight_df = flight_df.withColumn('CRS_ARR_HOUR', substring(convert_time_to_hour(\"CRS_ARR_TIME\"),0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "numeric_cols = [\"CRS_ELAPSED_TIME\",\"DISTANCE\"]\n",
    "\n",
    "op_carrier_indexer = StringIndexer(inputCol ='OP_CARRIER', outputCol = 'OP_CARRIER_INDEXED',handleInvalid=\"keep\")\n",
    "op_carrier_encoder = OneHotEncoder(inputCol ='OP_CARRIER_INDEXED', outputCol='OP_CARRIER_ENCODED')\n",
    "\n",
    "# op_carrier_fl_num_indexer = StringIndexer(inputCol ='OP_CARRIER_FL_NUM', outputCol = 'OP_CARRIER_FL_NUM_INDEXED',handleInvalid=\"keep\")\n",
    "# op_carrier_fl_num_encoder = OneHotEncoder(inputCol ='OP_CARRIER_FL_NUM_INDEXED', outputCol='OP_CARRIER_FL_NUM_ENCODED')\n",
    "\n",
    "origin_indexer = StringIndexer(inputCol ='ORIGIN', outputCol = 'ORIGIN_INDEXED',handleInvalid=\"keep\")\n",
    "origin_encoder = OneHotEncoder(inputCol ='ORIGIN_INDEXED', outputCol='ORIGIN_ENCODED')\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol ='DEST', outputCol = 'DEST_INDEXED',handleInvalid=\"keep\")\n",
    "dest_encoder = OneHotEncoder(inputCol ='DEST_INDEXED', outputCol='DEST_ENCODED')\n",
    "\n",
    "crs_dep_hour_indexer = StringIndexer(inputCol ='CRS_DEP_HOUR', outputCol = 'CRS_DEP_HOUR_INDEXED',handleInvalid=\"keep\")\n",
    "crs_dep_hour_encoder = OneHotEncoder(inputCol ='CRS_DEP_HOUR_INDEXED', outputCol='CRS_DEP_HOUR_ENCODED')\n",
    "\n",
    "# crs_arr_hour_indexer = StringIndexer(inputCol ='CRS_ARR_HOUR', outputCol = 'CRS_ARR_HOUR_INDEXED',handleInvalid=\"keep\")\n",
    "# crs_arr_hour_encoder = OneHotEncoder(inputCol ='CRS_ARR_HOUR_INDEXED', outputCol='CRS_ARR_HOUR_ENCODED')\n",
    "\n",
    "input_cols=[\n",
    "    'OP_CARRIER_ENCODED',\n",
    "    #'OP_CARRIER_FL_NUM_ENCODED',\n",
    "    'ORIGIN_ENCODED',\n",
    "    'DEST_ENCODED',\n",
    "    'CRS_DEP_HOUR_ENCODED'] + numeric_cols\n",
    "    #'CRS_ARR_HOUR_ENCODED'] + numeric_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = input_cols,\n",
    "    outputCol = 'features')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[op_carrier_indexer, \n",
    "                            op_carrier_encoder, \n",
    "                            #op_carrier_fl_num_indexer,\n",
    "                            #op_carrier_fl_num_encoder,\n",
    "                            origin_indexer,\n",
    "                            origin_encoder,\n",
    "                            dest_indexer,\n",
    "                            dest_encoder,\n",
    "                            crs_dep_hour_indexer,\n",
    "                            crs_dep_hour_encoder,\n",
    "                            #crs_arr_hour_indexer,\n",
    "                            #crs_arr_hour_encoder,\n",
    "                            assembler])\n",
    "\n",
    "pipelineModel = pipeline.fit(flight_df)\n",
    "model_df = pipelineModel.transform(flight_df)\n",
    "selectedCols = ['CANCELLED', 'features']# + cols\n",
    "model_df = model_df.select(selectedCols)\n",
    "model_df.printSchema()\n",
    "(train, test) = model_df.randomSplit([0.7, 0.3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'CANCELLED', maxIter=10)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: True)\n",
      "labelCol: label column name (default: label, current: CANCELLED)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: True)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lrModel.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48104394112215637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictionslr = lrModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionslr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.9965024926265236\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfclassifier = RandomForestClassifier(labelCol = 'CANCELLED', featuresCol = 'features')\n",
    "rfmodel = rfclassifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7561376351094455"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsrf = rfmodel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10,featuresCol = 'features', labelCol = 'CANCELLED')\n",
    "\n",
    "gbtModel = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6904067593838791"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsgbt = gbtModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsgbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
